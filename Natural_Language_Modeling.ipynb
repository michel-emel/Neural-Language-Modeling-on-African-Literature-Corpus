{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9m5a0synjNtP"
   },
   "source": [
    "## Zv7yPNR5W2_Assignment2_NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28S9O4N-jdF7"
   },
   "source": [
    "### 1- Concatenate all the fourth file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7kW2hkGWlhpa"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m randint\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcontractions\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import nltk\n",
    "import contractions\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU, Embedding\n",
    "from tensorflow.keras.models import load_model\n",
    "from pickle import dump, load\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KfmsfsRzhxXv"
   },
   "outputs": [],
   "source": [
    "#!pip install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylFkZuntn3Kn"
   },
   "source": [
    "### Upload all the four doc , clean them before concatenate into one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "id": "IUARJ67Jok67"
   },
   "outputs": [],
   "source": [
    "PATH_BASE = \"Data\"\n",
    "\n",
    "# Function to read a document from file\n",
    "def read_document(filename):\n",
    "    with open(filename, 'r', encoding='latin-1') as file:\n",
    "        # Use latin-1 encoding because there are some symbols that utf-8 isn't able to read\n",
    "        return file.read()\n",
    "\n",
    "def clean_text(text):\n",
    "  # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "     # Remove punctuation certain punstuation and digits\n",
    "  # Define punctuation to be remove we can see that [.!?] are not include we are going to use this so split the sentence further\n",
    "    punctuation = '\"#$%&\\'()*+/:;<=>@[\\\\]^_`{|};,'\n",
    "\n",
    "    text = re.sub(f\"[{punctuation}{string.digits}]\", \"\", text)\n",
    "\n",
    "     # Remove extra whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def expand_contractions(text):\n",
    "    words = text.split(\" \")\n",
    "    words_expand = []\n",
    "    for word in words:\n",
    "        if word in contractions.contractions_dict:\n",
    "            words_expand.append(contractions.fix(word))\n",
    "        else:\n",
    "            words_expand.append(word)\n",
    "\n",
    "    text_expand = ' '.join(words_expand)\n",
    "    return text_expand\n",
    "\n",
    "def test_clean(text):\n",
    "    # Expand contractions\n",
    "    expanded_text = expand_contractions(text)\n",
    "\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(expanded_text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# File names\n",
    "file_names = [\n",
    "    'A Man of the People.txt',\n",
    "    'Anthills of the Savannah.txt',\n",
    "    'Arrow of God.txt',\n",
    "    'Things Fall Apart.txt'\n",
    "]\n",
    "\n",
    "# Read and clean each document\n",
    "docs = [test_clean(read_document(os.path.join(PATH_BASE, file_name))) for file_name in file_names]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Concatenate the content of all cleaned documents\n",
    "concatenated_text_data = '\\n'.join(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m8wABtotofr3",
    "outputId": "5f2751fb-adc1-4d4b-d13a-1dcf7a53e6ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head of concatenated text:\n",
      "chinua achebe a man of the people first published in for chris chapter one no one can deny that chief the honourable m.a. nanga m.p. was the most approachable politician in the country. whether you asked in the city or in his home village anata they would tell you he was a man of the people. i have to admit this from the onset or else the story i am going to tell will make no sense. that afternoon he was due to address the staff and students of the anata grammar school where i was teaching at th\n",
      "\n",
      "End of concatenated text:\n",
      " states canada and nigeria. he is also the recipient of nigerias highest award for intellectual achievement the nigerian national merit award. at present mr. achebe lives with his wife in annandale new york where they both teach at bard college. they have four children. also by chinua achebe anthills of the savannah arrow of god girls at war and other stories a man of the people no longer at ease nonfiction hopes and impediments selected essays the trouble with nigeria poetry beware soul brother\n"
     ]
    }
   ],
   "source": [
    " #Display the head of the concatenated_text\n",
    "head_text = concatenated_text_data[:500]  # Change 100 to the desired number of characters\n",
    "print(\"Head of concatenated text:\")\n",
    "print(head_text)\n",
    "\n",
    "# Display the end of the concatenated_text\n",
    "end_text = concatenated_text_data[-500:]  # Change 100 to the desired number of characters\n",
    "print(\"\\nEnd of concatenated text:\")\n",
    "print(end_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrBjVIY_to9Z"
   },
   "source": [
    "We see that the beginning of the result file is the the head of first file and the end is the tail of the last file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsW4Ucuu0BuP"
   },
   "source": [
    "## 2- split the data into train( train set content the validation set ) and test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "id": "yzwJIq_0ofoW"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bq4Dd1lWMtKh",
    "outputId": "431e70bb-866d-4cb2-8fc0-15d170e9473f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chinua achebe a man of the people first published in for chris chapter one no one can deny that chief the honourable m',\n",
       " 'a',\n",
       " ' nanga m',\n",
       " 'p',\n",
       " ' was the most approachable politician in the country',\n",
       " ' whether you asked in the city or in his home village anata they would tell you he was a man of the people',\n",
       " ' i have to admit this from the onset or else the story i am going to tell will make no sense',\n",
       " ' that afternoon he was due to address the staff and students of the anata grammar school where i was teaching at the time',\n",
       " ' but as usual in those highly political times the villagers moved in and virtually took over',\n",
       " ' the assembly hall must have carried well over thrice its capacity']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the concatenated text into sentences using multiple punctuation marks\n",
    "sentences_by_punctuation = re.split(r'[.!?]', concatenated_text_data)\n",
    "#print the 10 first sentences\n",
    "sentences_by_punctuation[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVONEe1lTsXO"
   },
   "source": [
    "We can see that the sentence are very well define. Just at the beginning that we have a small error because the name of the author is write in the article with the dot. The idea behind operation of the entire data it to avoid during the splitting ( train , test and validation) the sentence that don't make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "id": "N-tklngeTrPI"
   },
   "outputs": [],
   "source": [
    "# we split our data 70% for train 15% for test and 15% for validation\n",
    "train_data, global_test_data = train_test_split(sentences_by_punctuation, test_size=0.30, random_state=42)\n",
    "\n",
    "val_data, test_data = train_test_split(global_test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Let us join the texts to make a file as beginning with sentence\n",
    "train_text = ' '.join(train_data)\n",
    "val_text = ' '.join(val_data)\n",
    "test_text = ' '.join(test_data)\n",
    "\n",
    "# We save our data\n",
    "with open('train_text.txt', 'w') as file:\n",
    "    file.write(train_text)\n",
    "\n",
    "with open('val_text.txt', 'w') as file:\n",
    "    file.write(val_text)\n",
    "\n",
    "with open('test_text.txt', 'w') as file:\n",
    "    file.write(test_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "9H1e3IoyU4FI",
    "outputId": "920c3d4f-5e8a-449a-c37d-50046393a143"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' he sat with a short wooden-headed knife between two heaps of yams  he was still looking at his palm'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each data we get the test file that we can print the first 100 caracter\n",
    "train_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' but he was struck as most people were by okonkwos brusqueness in dealing with less successful men  '"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aGi8v92HaCE1",
    "outputId": "60859959-a3fd-46d1-dbd0-4a23a322518e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/student23/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk import ngrams\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "id": "vdfrxEFFZ6L1"
   },
   "outputs": [],
   "source": [
    "# Tokenize the text.\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent)))\n",
    "                  for sent in sent_tokenize(train_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "beSCN9g2Z6Ip",
    "outputId": "93ec9465-bb42-42f3-fc41-f656586afa94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we get a list of one element which is the list of the word\n",
    "tokenized_text[0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "t1NcUOm8Z6Fk"
   },
   "outputs": [],
   "source": [
    "# Preprocess the tokenized text for 2-grams language modelling\n",
    "n = 2 # n gram\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
    "# Convert to lists\n",
    "train_data_list = list(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-LFlo_h9cYTw"
   },
   "source": [
    "## Training an bi-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "id": "CbNk-BNVWT3i"
   },
   "outputs": [],
   "source": [
    "# initialize MLE(maximun likelihood) model - MLE model, creates an empty vocabulary\n",
    "model = MLE(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gxzIhN9BcbOb",
    "outputId": "b6a36333-5ab6-4155-89ab-3e7fa19bf280"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before the train the vocab is o\n",
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJ8NLfzicbLF",
    "outputId": "58f0fd06-8a15-4a42-ae8a-786e466fb643"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 12754 items>\n"
     ]
    }
   ],
   "source": [
    "# traine model\n",
    "model.fit(train_data_list, padded_sents)\n",
    "print(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j4oMQCpscbID",
    "outputId": "84d79c56-2ae8-4613-e783-2c50945070de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12754"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after traitement the voab is different\n",
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bzchq57ecbFV",
    "outputId": "37944d38-b20d-4ec3-c74a-2bc275eaea00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('he', 'sat', 'with', 'a', 'short', 'wooden-headed', 'knife', 'between', 'two', 'heaps', 'of', 'yams', 'he', 'was', 'still', 'looking', 'at', 'his', 'palm', 'he')\n"
     ]
    }
   ],
   "source": [
    "# The set of word in our model\n",
    "print(model.vocab.lookup(tokenized_text[0][0:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Reu3Lj8cbCo",
    "outputId": "490d4e77-f7c1-4da0-f2e8-407bc429c715"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193802"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocab.lookup(tokenized_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A2t1nhl_ca_1",
    "outputId": "50008c23-8554-49b5-bcc1-91f8b64690ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('language', 'is', 'never', 'random', '<UNK>', '<UNK>')\n"
     ]
    }
   ],
   "source": [
    "# the model automatically replace words not in the vocabulary with `<UNK>`.\n",
    "print(model.vocab.lookup('language is never random unseenword .'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ac-HV3qew2G"
   },
   "source": [
    "## Explore our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G17FrL1Gf5Ct",
    "outputId": "69184c34-1f36-45b9-80f5-124566242325"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 2 ngram orders and 387607 ngrams>\n"
     ]
    }
   ],
   "source": [
    "print(model.counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counts the number of bigram of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "exDdxfxxewAs",
    "outputId": "12d895d7-7dd7-4f99-f34b-f6fdddc0846d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 50.00103199174407\n"
     ]
    }
   ],
   "source": [
    "# Evaluate perplexity\n",
    "perplexity = model.perplexity(\"a\")\n",
    "print(\"Perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TNmMysiSfGhI",
    "outputId": "79b68e2e-c542-43ac-ac3d-472cce9a5be5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 4023.397757057321\n"
     ]
    }
   ],
   "source": [
    "# Evaluate perplexity\n",
    "perplexity = model.perplexity(\"heaps\")\n",
    "print(\"Perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Notice the perplexity of common word is small and for the rare word is high. This can be explain because the perplexity is inverse proportionnal to the  probability (frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P2B1XsHNfNFz",
    "outputId": "a1257ff3-e5c0-4f96-9afc-988a8d3bacf9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02097902097902098"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability of 'has' given 'has'\n",
    "model.score('he', 'has'.split())  # P('has'|'he') [he has]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDYjSXoC88l_"
   },
   "source": [
    "## Evaluate the perplexity of the test set with our bigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sbLej8tAtvfV",
    "outputId": "17e0450b-1a93-4797-82e4-ab9e1875032a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: inf\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "tokenized_test_text = [list(map(str.lower, word_tokenize(sent)))\n",
    "                  for sent in sent_tokenize(train_text)]\n",
    "\n",
    "test_ngrams = list(pad_both_ends(ngrams(tokenized_test_text[0], n),n))\n",
    "\n",
    "\n",
    "# Compute perplexity for the test set\n",
    "test_perplexity = model.perplexity(test_ngrams)\n",
    "print(\"Test Perplexity:\", test_perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49F8bJsryM1D",
    "outputId": "9747b464-08ef-4b37-d69a-f0f025f4c39f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('he', 'sat'), ('sat', 'with'), ('with', 'a')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A test gra is a list of two words\n",
    "test_ngrams[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNZqMEO8w9ZS"
   },
   "source": [
    "**Comment:** Our perplexity if infinity mean that this bigram model is very worse to predict on unseen data. So The probability of the test set is too small. This can be explain by the lack of data. The data is not enough to train very well the model to get good result for perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ahPyxO2tvhUE",
    "outputId": "1f8cfeac-f349-4c1b-e352-2c9387463dda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of <s>: 0\n"
     ]
    }
   ],
   "source": [
    "for ngram in test_ngrams[0:10]:\n",
    "    prob = model.score(ngram[-1], ngram[:-1])\n",
    "    if prob == 0 :\n",
    "        print(f\"Probability of {ngram}: {prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOW5azY870ZZ"
   },
   "source": [
    "We investigate of there somme bigramm with zero probability.We realize that there is not bigram in our test set we zero probability.So here we don't need smothing method here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-435-d00a48ac548c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_ngrams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Probability of {ngram}: {prob}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'score'"
     ]
    }
   ],
   "source": [
    "for ngram in test_ngrams[0:10]:\n",
    "    prob = model.score(ngram[-1], ngram[:-1])\n",
    "    print(f\"Probability of {ngram}: {prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This previous code display the probability of 10 frist bigram. we can notice that must probability is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nH8GJwPeRHq"
   },
   "source": [
    "## Generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "LAH1W41rca4l"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "detokenize = TreebankWordDetokenizer().detokenize\n",
    "\n",
    "def generate_sent(model, num_words, random_seed=42):\n",
    "    \"\"\"Generate text method  \"\"\"\n",
    "    content = []\n",
    "    for token in model.generate(num_words, random_seed=random_seed):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "    return detokenize(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "c_FssKj8ca1Y",
    "outputId": "2395bba3-f0c5-4a8a-a510-e59c7bc08e5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'did he sat nearest to help look out their families let her people and sneered why there last time as the day or'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with our model we try to generate a text for 23 word ramdomly.\n",
    "generate_sent(model,23, random_seed=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoombUAb8gCC"
   },
   "source": [
    "We try test generation many time and we realize that there word generate doesn't have reel semantic or any strong grammatical construction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3ynGCeu9fYP"
   },
   "source": [
    "## Compute the Blue score\n",
    " Here we evaluate using the reference sentence and using the entire test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_e74VHMJIRd",
    "outputId": "a7fa8f16-097f-4833-f116-36fe92bd4456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence: ['he', 'clarke', 'minister', 'seeing', 'them', 'that', 'or', 'the', 'knee', 'visits', 'great', 'animals', 'him', 'to', 'mean', 'sleepy', 'stop', 'a', 'of', 'on', 'to']\n",
      "BLEU Score: 0.00913442366656447\n"
     ]
    }
   ],
   "source": [
    "# Define a function to generate a sample sentence using the n-gram model\n",
    "def generate_ngram_sentence(seed_word, n, model, max_length=20):\n",
    "    sentence = [seed_word]\n",
    "    for _ in range(max_length):\n",
    "        next_word = model.generate()\n",
    "        sentence.append(next_word)\n",
    "    return sentence\n",
    "\n",
    "# Generate a sample sentence using the trained model\n",
    "generated_sentence = generate_ngram_sentence(\"he\", n, model)\n",
    "print(\"Generated Sentence:\",generated_sentence)\n",
    "\n",
    "# Define the reference sentence\n",
    "reference_sentence = \"This is a sample sentence.\"\n",
    "reference_tokens = nltk.word_tokenize(reference_sentence.lower())\n",
    "\n",
    "# Convert generated sentence to lowercase\n",
    "generated_tokens = [str(token).lower() for token in generated_sentence]\n",
    "\n",
    "# Calculate BLEU score for the generated sentence\n",
    "bleu_score = sentence_bleu([reference_tokens], generated_tokens, smoothing_function=SmoothingFunction().method1)\n",
    "print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2NFNYHQ-iRq",
    "outputId": "006a9e2a-9403-45e9-a102-f99b4ba2e446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 2.9945485131931132e-164\n"
     ]
    }
   ],
   "source": [
    "# ## Function to generate a sentence using the n-gram model\n",
    "def generate_ngram_sentence(seed_word, n, model, max_length=223471):\n",
    "    sentence = [seed_word]\n",
    "    for _ in range(max_length):\n",
    "        next_word = model.generate()\n",
    "        sentence.append(next_word)\n",
    "    return sentence\n",
    "\n",
    "# Use the test set for BLEU score calculation\n",
    "#test_reference_sentence = \"This is a test sentence.\"  # Replace with an actual sentence from your test set\n",
    "test_reference_tokens = nltk.word_tokenize(test_text)\n",
    "\n",
    "# Generate a sample sentence using the trained model\n",
    "generated_sentence = generate_sent(model,20, random_seed=42)\n",
    "#print(\"Generated Sentence:\", generated_sentence)\n",
    "\n",
    "# Calculate BLEU score for the generated sentence using the test reference\n",
    "generated_tokens = [str(token) for token in generated_sentence]\n",
    "bleu_score = sentence_bleu([test_reference_tokens], generated_tokens, smoothing_function=SmoothingFunction().method1)\n",
    "print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKvfXgDcQe8e"
   },
   "source": [
    "### comment \n",
    "The Blue score of bigram model is very worse so to to better performance we can use The neural network lamguage modeling that will try to implemnet at the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHsFdykQQiPl"
   },
   "source": [
    "### Implement RNN model (one word in one word out)\n",
    "Model properties:\n",
    "\n",
    "- model has an embedding layer to learn the word embedding\n",
    "- he input sequence contains a single word therefore input_length = 1\n",
    "- the model has a single LSTM layer with 50 units\n",
    "- output layer has a softmax activation function and is comprised of one neuron for each word in the vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "id": "Hd7msKC5JIC5"
   },
   "outputs": [],
   "source": [
    "# integer encode text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([train_text])\n",
    "encoded = tokenizer.texts_to_sequences([train_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text to sequence for the test_text to evaluate the model\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_seq = [num if sublist else 0 for sublist in X_test_seq for num in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150971"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194567"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nFpKZ14uJH_A",
    "outputId": "bc330704-86f0-4749-ddf2-66235da4b374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 12072\n"
     ]
    }
   ],
   "source": [
    "# determine the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZkIM43lRgVH"
   },
   "source": [
    "Create sequences of words to fit the model with one word as input and one word as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7FhQKkIRRmQT",
    "outputId": "f1cf4c82-3fb1-4477-fc6c-e2012c76c475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 194566\n"
     ]
    }
   ],
   "source": [
    "# create word -> word sequences for the train\n",
    "sequences = list()\n",
    "for i in range(1, len(encoded)):\n",
    "    sequence = encoded[i-1:i+1]\n",
    "    sequences.append(sequence)\n",
    "\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 150970\n"
     ]
    }
   ],
   "source": [
    "# create word -> word sequences for the test\n",
    "test_sequence = list()\n",
    "for i in range(1, len(X_test_seq)):\n",
    "    test_sequence1 = X_test_seq[i-1:i+1]\n",
    "    test_sequence.append(test_sequence1)\n",
    "\n",
    "print('Total Sequences: %d' % len(test_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3YnxuxbJRmMA",
    "outputId": "cc3e2903-764c-49eb-dabd-03f6584d0184",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length: 2\n"
     ]
    }
   ],
   "source": [
    "# pad input sequences for train\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length: 2\n"
     ]
    }
   ],
   "source": [
    "# pad input sequences for test\n",
    "# Pad sequences to the maximum length\n",
    "max_length_test = max(len(seq) for seq in test_sequence)\n",
    "padded_sequences_test = pad_sequences(test_sequence, maxlen=max_length_test, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "id": "BCKeG_a2RmIe"
   },
   "outputs": [],
   "source": [
    "# split sequence into input X and output y\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,0], sequences[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate test into input and output\n",
    "\n",
    "# Convert the sequences to a 2D numpy array\n",
    "sequences_array_test = np.array(padded_sequences_test)\n",
    "\n",
    "X_test, y_test = sequences_array_test[:, :-1], sequences_array_test[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape((X_test.shape[0], 1))\n",
    "y_test = y_test.reshape((y_test.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150970, 1), (150970, 1))"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape((X.shape[0], 1))\n",
    "y = y.reshape((y.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCy8-noPRmFI",
    "outputId": "a1223a86-4d7a-4798-c57c-887ba2a82ace"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((194566, 1), (194566, 1))"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8Jd_OltR8Cr"
   },
   "source": [
    "To fit a model to predict a probability distribution across all words in the vocabulary. We need to turn the output element (y) from a single integer into a one hot encoding with a 0 for every word in the vocabulary and a 1 for the actual word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "id": "PawcG5HmRmBq"
   },
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "id": "VKmiTPOlRl95"
   },
   "outputs": [],
   "source": [
    "# One-hot encode the output variable of the test\n",
    "y_test = to_categorical(y_test, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150970, 12072)"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXoxBXJBSF32"
   },
   "source": [
    "### Train neural language model (one word in one word out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "id": "l8cE0lhZRl5x"
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "def define_model(vocab_size):\n",
    "    \"\"\"Define the Deep learning \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 10))\n",
    "    model.add(LSTM(50, input_shape=(1, 10), return_sequences=True)) \n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "    # compile network\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vocab_size):\n",
    "    \"\"\"Define the Deep learning \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 10))  # Remove input_length argument\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(vocab_size, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Print model summary\n",
    "    model.summary()\n",
    "    \n",
    "    # Build the model\n",
    "    model.build((None,10))  # Input shape with None indicates variable sequence length\n",
    "    \n",
    "    # Plot the model\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugxEGEk9Rl0I",
    "outputId": "a65295f8-82fd-40b3-bdcf-bb07f1f927d9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_24\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_24\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_24 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_22 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install graphviz (see instructions at https://graphviz.gitlab.io/download/) for `plot_model` to work.\n",
      "Epoch 1/10\n",
      "\u001b[1m6081/6081\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 12ms/step - accuracy: 0.0540 - loss: 7.0675\n",
      "Epoch 2/10\n",
      "\u001b[1m6081/6081\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 12ms/step - accuracy: 0.0748 - loss: 6.2620\n",
      "Epoch 3/10\n",
      "\u001b[1m6081/6081\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 12ms/step - accuracy: 0.1040 - loss: 5.9341\n",
      "Epoch 4/10\n",
      "\u001b[1m6081/6081\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 13ms/step - accuracy: 0.1145 - loss: 5.7703\n",
      "Epoch 5/10\n",
      "\u001b[1m6081/6081\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 13ms/step - accuracy: 0.1226 - loss: 5.6490\n",
      "Epoch 6/10\n",
      "\u001b[1m6081/6081\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 12ms/step - accuracy: 0.1277 - loss: 5.5564\n",
      "Epoch 7/10\n",
      "\u001b[1m6081/6081\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 12ms/step - accuracy: 0.1322 - loss: 5.4930\n",
      "Epoch 8/10\n",
      "\u001b[1m6081/6081\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 12ms/step - accuracy: 0.1377 - loss: 5.4253\n",
      "Epoch 9/10\n",
      "\u001b[1m6081/6081\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 12ms/step - accuracy: 0.1419 - loss: 5.3685\n",
      "Epoch 10/10\n",
      "\u001b[1m6081/6081\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 12ms/step - accuracy: 0.1470 - loss: 5.3049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fdb09684b80>"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(vocab_size)\n",
    "\n",
    "model.fit(X, y, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of our model increase as epoch increase and the loss decrease. we can say that our model learn well and will be better with the high number of epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename='model.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realise that the table show the summary of the architecture of our model is not display (somme package needed) so we get the model image usin Golab(The model isn't able to train in Colab but we can see this table clearly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjqLBBxRRldZ"
   },
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer and truncate sequences to a fixed length\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        yhat = np.argmax(model.predict(encoded), axis=-1)\n",
    "\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = re.split(r'[.!?]', concatenated_text_data)\n",
    "seq_length = len(sequence[0].split()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chinua achebe a man of the people first published in for chris chapter one no one can deny that chief the honourable m'"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in the morning as there was no sign of life in her hut her husband pushed open the door and peeped in\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "idx = randint(0,len(sequence))# take random line in the full concat file\n",
    "seed_text = sequence[idx]\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 547ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "generated text\n",
      "the sleazy under the odorous the other other people people\n"
     ]
    }
   ],
   "source": [
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 10)\n",
    "print('generated text')\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "The sequence generate by our Language Modeling is a little bit better than the bigram. And this can be better improve by parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Perplexity of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4718/4718\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 6ms/step - accuracy: 0.0387 - loss: 9.0948\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Loss of our test set is:  9.095501899719238\n"
     ]
    }
   ],
   "source": [
    "print(\"The Loss of our test set is: \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of our test set is:  0.03832549601793289\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy of our test set is: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate perplexity \n",
    "perplexity = 2 ** test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of our test set is:  547.0397656902501\n"
     ]
    }
   ],
   "source": [
    "print(\"The perplexity of our test set is: \", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment \n",
    "We notice the complexity is high so the model doesn't perform very well. Here we can see the with more epoch we will get better performance but due to the lack of computer ressources we don't train it with a large epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Bleue of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aahed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0      a\n",
       "1     aa\n",
       "2    aaa\n",
       "3    aah\n",
       "4  aahed"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#upload the english word to be sure that the word generate exit in english vocabulary\n",
    "Word_Alpha = pd.read_csv(\"words_alpha.txt\",header= None, delimiter = \"\\t\")\n",
    "Word_Alpha.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the word alphabet to list\n",
    "english_words = Word_Alpha.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The seed_test is : \n",
      "\n",
      "Generate sequence is :  and and the sleazy searching with the odorous the other\n",
      "BLEU Score: 0.021105340631872645\n"
     ]
    }
   ],
   "source": [
    "# Function to generate a sequence given a seed text\n",
    "def generate_sequence(seed_text, model, max_length, tokenizer):\n",
    "    \"\"\"\n",
    "    Generate a sequence of words using a trained LSTM model.\n",
    "\n",
    "    Parameters:\n",
    "    - seed_text (str): The initial text to start the sequence.\n",
    "    - model (Sequential): The trained LSTM model.\n",
    "    - max_length (int): The maximum length of the generated sequence.\n",
    "    - tokenizer (Tokenizer): Tokenizer used for text encoding.\n",
    "    - english_words (set): A set of English words for validation.\n",
    "\n",
    "    Returns:\n",
    "    - generated_sequence (str): The generated sequence of words.\n",
    "    \"\"\"\n",
    "    generated_sequence = seed_text\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        encoded_seed = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        padded_seed = pad_sequences([encoded_seed], maxlen=max_length-1, padding='pre')\n",
    "        \n",
    "        # Use predict to get the probabilities of each word\n",
    "        predicted_probs = model.predict(padded_seed, verbose=0)\n",
    "        \n",
    "        # Choose the word with the highest probability\n",
    "        predicted_word_index = np.argmax(predicted_probs)\n",
    "        predicted_word = tokenizer.index_word[predicted_word_index]\n",
    "        \n",
    "        seed_text += ' ' + predicted_word\n",
    "        generated_sequence += ' ' + predicted_word\n",
    "    \n",
    "    return generated_sequence# in english_words\n",
    "\n",
    "\n",
    "# Sample seed text\n",
    "# select a seed text\n",
    "idx = randint(0,len(sequence))# take random line in the full concat file\n",
    "seed_text = sequence[idx]\n",
    "print(\"The seed_test is : \"+ seed_text + '\\n')\n",
    "\n",
    "# Generate a sequence using the trained LSTM model\n",
    "max_length = 10  # Adjust this based on the maximum length of sequences in your training data\n",
    "generated_sequence = generate_sequence(seed_text, model, max_length, tokenizer)#,english_words)\n",
    "\n",
    "# Sample reference sentence for BLEU score calculation\n",
    "reference_sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Tokenize the reference sentence and the generated sequence\n",
    "reference_tokens = nltk.word_tokenize(reference_sentence)\n",
    "generated_tokens = nltk.word_tokenize(generated_sequence)\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = sentence_bleu([reference_tokens], generated_tokens, smoothing_function=SmoothingFunction().method1)\n",
    "print(\"Generate sequence is :\",generated_sequence)\n",
    "print(\"BLEU Score:\", bleu_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment \n",
    "The Blue score near to 1 mean that the model perform well. The Blue score here is far from 1 and near to zero so the model doesn't have the good performance. To enhance the performance of our model we need to add the epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model multiple input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the input sequence has high len than previous(which was just 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 193751\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 50 +1\n",
    "sequences = list()\n",
    "# we use the token of the tokenized_text previous in ngram model represented the tokens of the train set \n",
    "for i in range(length, len(tokenized_text[0])):\n",
    "    # select sequence of tokens\n",
    "    seq = tokenized_text[0][i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    sequences.append(line)\n",
    "\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "print(len(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we use the tokens of our train to generate 193751 sequences of 51 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he sat with a short wooden-headed knife between two heaps of yams he was still looking at his palm he knew that he was a fierce fighter but that year-had been enough to break the heart of a lion but although he thought for a long time he found no answer',\n",
       " 'sat with a short wooden-headed knife between two heaps of yams he was still looking at his palm he knew that he was a fierce fighter but that year-had been enough to break the heart of a lion but although he thought for a long time he found no answer eei',\n",
       " 'with a short wooden-headed knife between two heaps of yams he was still looking at his palm he knew that he was a fierce fighter but that year-had been enough to break the heart of a lion but although he thought for a long time he found no answer eei what',\n",
       " 'a short wooden-headed knife between two heaps of yams he was still looking at his palm he knew that he was a fierce fighter but that year-had been enough to break the heart of a lion but although he thought for a long time he found no answer eei what he',\n",
       " 'short wooden-headed knife between two heaps of yams he was still looking at his palm he knew that he was a fierce fighter but that year-had been enough to break the heart of a lion but although he thought for a long time he found no answer eei what he ended']"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A sequence here is the chunck of 50 words\n",
    "sequences[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193802"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our corpus of train has 1025656 tokens and we make a sequence of 50 words\n",
    "len(tokenized_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer_M = Tokenizer()\n",
    "tokenizer_M.fit_on_texts(sequences)\n",
    "sequences_token = tokenizer_M.texts_to_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12072\n"
     ]
    }
   ],
   "source": [
    "# vocabulary size\n",
    "vocab_size2 = len(tokenizer_M.word_index) + 2 #to be the samelength as the previous\n",
    "print(vocab_size2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just notice that the vocabulary is almost the same as previous for one input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X2 Shape: (193751, 61)\n",
      "y2 Shape: (193751, 12072)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Pad sequences to the maximum length\n",
    "max_length = max(len(seq) for seq in sequences_token)\n",
    "padded_sequences = pad_sequences(sequences_token, maxlen=max_length, padding='pre')\n",
    "\n",
    "# Convert the sequences to a 2D numpy array\n",
    "sequences_array = np.array(padded_sequences)\n",
    "\n",
    "# separate into input and output\n",
    "X2, y2 = sequences_array[:, :-1], sequences_array[:, -1]\n",
    "\n",
    "# One-hot encode the output variable\n",
    "y2 = to_categorical(y2, num_classes=vocab_size2)\n",
    "\n",
    "print('X2 Shape:', X2.shape)\n",
    "print('y2 Shape:', y2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply LSTM we need to the input to be in the same shape , that why we add padding in each input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(vocab_size):\n",
    "    \"\"\"Define the Deep learning \"\"\"\n",
    "    model2 = Sequential()\n",
    "    model2.add(Embedding(vocab_size, 50))\n",
    "    model2.add(LSTM(100, return_sequences=True))\n",
    "    model2.add(LSTM(100))\n",
    "    model2.add(Dense(100, activation='relu'))\n",
    "    model2.add(Dense(vocab_size, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Print model summary\n",
    "    model2.summary()\n",
    "    \n",
    "    # Build the model\n",
    "    model2.build((None,61))  # Input shape with None indicates variable sequence length\n",
    "    \n",
    "    # Plot the model\n",
    "    plot_model(model2, to_file='model.png', show_shapes=True)\n",
    "\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_25\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_25\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_25 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_23 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_24 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install graphviz (see instructions at https://graphviz.gitlab.io/download/) for `plot_model` to work.\n",
      "Epoch 1/5\n",
      "\u001b[1m6055/6055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 81ms/step - accuracy: 0.0591 - loss: 6.7258\n",
      "Epoch 2/5\n",
      "\u001b[1m6055/6055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m533s\u001b[0m 86ms/step - accuracy: 0.0934 - loss: 6.0576\n",
      "Epoch 3/5\n",
      "\u001b[1m6055/6055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m529s\u001b[0m 87ms/step - accuracy: 0.1082 - loss: 5.8237\n",
      "Epoch 4/5\n",
      "\u001b[1m6055/6055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m532s\u001b[0m 88ms/step - accuracy: 0.1167 - loss: 5.6341\n",
      "Epoch 5/5\n",
      "\u001b[1m6055/6055\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m555s\u001b[0m 87ms/step - accuracy: 0.1223 - loss: 5.5002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fdae7739c70>"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model2 = define_model(vocab_size2)\n",
    "\n",
    "model2.fit(X2, y2, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add epoch to better performance and also computer ressources. at 5 epoch we get 12% of accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the test with this multiple output\n",
    " We use the function that we write before to generate the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = re.split(r'[.!?]', concatenated_text_data)\n",
    "seq_length = len(sequence[0].split()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " they made single mounds of earth in straight lines all over the field and sowed the yams in them\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "idx = randint(0,len(sequence))# take random line in the full concat file\n",
    "seed_text = sequence[idx]\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "generated text\n",
      "and the man of the man of the man of the man of\n"
     ]
    }
   ],
   "source": [
    "# generate new text\n",
    "generated = generate_seq(model2, tokenizer_M, seq_length, seed_text, 13)\n",
    "print('generated text')\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment: \n",
    "Our Model generate the word or sequence that not really make sens( grammatically not real or correct). This can be improve by train the model in more epoch, here we just try 2 epoch because we was't have enough capacity of RAM to run and train the model well.He can remarq how far the accuracy increase in the second epoch so with many epoch we can get good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation du model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss2, test_accuracy2 = model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Loss of our test set is: \", test_loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The accuracy of our test set is: \", test_accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate perplexity \n",
    "perplexity2 = 2 ** test_loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The perplexity of our test set is: \", perplexity2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Blue Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a sequence given a seed text\n",
    "def generate_sequence(seed_text, model, max_length, tokenizer):\n",
    "    \"\"\"\n",
    "    Generate a sequence of words using a trained LSTM model.\n",
    "\n",
    "    Parameters:\n",
    "    - seed_text (str): The initial text to start the sequence.\n",
    "    - model (Sequential): The trained LSTM model.\n",
    "    - max_length (int): The maximum length of the generated sequence.\n",
    "    - tokenizer (Tokenizer): Tokenizer used for text encoding.\n",
    "    - english_words (set): A set of English words for validation.\n",
    "\n",
    "    Returns:\n",
    "    - generated_sequence (str): The generated sequence of words.\n",
    "    \"\"\"\n",
    "    generated_sequence = seed_text\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        encoded_seed = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        padded_seed = pad_sequences([encoded_seed], maxlen=max_length-1, padding='pre')\n",
    "        \n",
    "        # Use predict to get the probabilities of each word\n",
    "        predicted_probs = model.predict(padded_seed, verbose=0)\n",
    "        \n",
    "        # Choose the word with the highest probability\n",
    "        predicted_word_index = np.argmax(predicted_probs)\n",
    "        predicted_word = tokenizer.index_word[predicted_word_index]\n",
    "        \n",
    "        seed_text += ' ' + predicted_word\n",
    "        generated_sequence += ' ' + predicted_word\n",
    "    \n",
    "    return generated_sequence# in english_words\n",
    "\n",
    "\n",
    "# Sample seed text\n",
    "# select a seed text\n",
    "idx = randint(0,len(sequence))# take random line in the full concat file\n",
    "seed_text = sequence[idx]\n",
    "print(\"The seed_test is : \"+ seed_text + '\\n')\n",
    "\n",
    "# Generate a sequence using the trained LSTM model\n",
    "max_length = 10  # Adjust this based on the maximum length of sequences in your training data\n",
    "generated_sequence = generate_sequence(seed_text, model2, max_length, tokenizer)#,english_words)\n",
    "\n",
    "# Sample reference sentence for BLEU score calculation\n",
    "reference_sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Tokenize the reference sentence and the generated sequence\n",
    "reference_tokens = nltk.word_tokenize(reference_sentence)\n",
    "generated_tokens = nltk.word_tokenize(generated_sequence)\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = sentence_bleu([reference_tokens], generated_tokens, smoothing_function=SmoothingFunction().method1)\n",
    "print(\"Generate sequence is :\",generated_sequence)\n",
    "print(\"BLEU Score:\", bleu_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Summary, given 4 txt data, we concat those file, then split then into test, validation and test after some preprocessing(remove unseless punctuation, space, normalize this). We applied 3 several laguage model bigram (n=2), LSTM for one input, LSTM for multiple input."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
